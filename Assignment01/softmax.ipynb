{"cells":[{"cell_type":"code","execution_count":null,"id":"161a3aa0","metadata":{"id":"161a3aa0"},"outputs":[],"source":["# This mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# TODO: Enter the foldername in your Drive where you have saved the unzipped\n","# assignment folder, e.g. 'Deep_learning_kursus/assignments/assignment1/' such \n","#that e.g.: FOLDERNAME = 'Deep_learningE22/assignments/assignment1/'\n","\n","FOLDERNAME = None\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# Now that we've mounted your Drive, this ensures that\n","# the Python interpreter of the Colab VM can load\n","# python files from within it.\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))"]},{"cell_type":"markdown","id":"26fdca1c","metadata":{"tags":["pdf-title"],"id":"26fdca1c"},"source":["# Softmax exercise\n","\n","*Complete this exercise and enclude results and output graphs along with desribtions of the model and development method in the assignment submission.\n","\n","This exercise is analogous to the SVM exercise. You will:\n","\n","- implement a fully-vectorized **loss function** for the Softmax classifier\n","- implement the fully-vectorized expression for its **analytic gradient**\n","- **check your implementation** with numerical gradient\n","- use a validation set to **tune the learning rate and regularization** strength\n","- **optimize** the loss function with **SGD**\n","- **visualize** the final learned weights\n"]},{"cell_type":"code","execution_count":null,"id":"1f12280f","metadata":{"tags":["pdf-ignore"],"id":"1f12280f"},"outputs":[],"source":["import random\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# This is a bit of magic to make matplotlib figures appear inline in the notebook\n","# rather than in a new window.\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","\n","\n","# Some more magic so that the notebook will reload external python modules;\n","# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":null,"id":"ab106c82","metadata":{"tags":["pdf-ignore"],"id":"ab106c82"},"outputs":[],"source":["# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n","try:\n","   del HAM_train_data\n","   del HAM_val_data\n","   del HAM_test_data\n","   print('Clear previously loaded data.')\n","except:\n","   pass\n","\n","\n","# Load the training and tuning datasets.\n","%cd /content/drive/My\\ Drive/$FOLDERNAME/datasets/HAM_modified/\n","HAM_train_data = np.load('HAM_train.npz',allow_pickle=True)\n","HAM_val_data = np.load('HAM_val.npz',allow_pickle=True)\n","HAM_test_data = np.load('HAM_test.npz',allow_pickle=True)\n","%cd /content/drive/My\\ Drive/$FOLDERNAME\n","\n","#load and print header to view some info on the data and example data\n","header = HAM_train_data['header']\n","print('\\n')\n","print(header)\n","print('\\n')\n","print(HAM_train_data['images'][0].shape,HAM_train_data['meta'][0][:],'label: ',HAM_train_data['labels'][0])\n","print('\\n')\n","X_train, y_train, train_meta = HAM_train_data['images'], HAM_train_data['labels'], HAM_train_data['meta']\n","\n","X_val, y_val, val_meta = HAM_val_data['images'], HAM_val_data['labels'], HAM_val_data['meta']\n","\n","X_test, y_test, test_meta = HAM_test_data['images'], HAM_test_data['labels'], HAM_test_data['meta']\n","\n","\n","# As a sanity check, we print out the size of the training and test data.\n","print('Training data shape: ', X_train.shape)\n","print('Training labels shape: ', y_train.shape)\n","print('Training meta info shape: ', train_meta.shape)\n","print('\\n')\n","print('Val data shape: ', X_val.shape)\n","print('Val labels shape: ', y_val.shape)\n","print('Val meta info shape: ', val_meta.shape)\n","print('\\n')\n","print('Test data shape: ', X_test.shape)\n","print('Test labels shape: ', y_test.shape)\n","print('Test meta info shape: ', test_meta.shape)"]},{"cell_type":"code","source":["# Visualize some examples from the dataset.\n","# We show a few examples of training images from each class.\n","# we are going to be classifying three different types of \n","# skin lesion on images sampled and modified from the HAM10000 data-set:\n","# https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T \n","\n","\n","class_dict = {'bkl':'benign keratosis-like lesions', \n","               'vasc':'vascular lesions', \n","               'nv':'melanocytic nevi'}\n","\n","\n","num_classes = len(class_dict.keys())\n","\n","print('Number of classes = {}'.format(num_classes))\n","\n","samples_per_class = 7\n","for y, cls in enumerate(class_dict.keys()):\n","    idxs = np.where(train_meta[:] == cls)[0]\n","    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n","    for i, idx in enumerate(idxs):\n","        plt_idx = i * num_classes + y + 1\n","        plt.subplot(samples_per_class, num_classes, plt_idx)\n","        plt.imshow(X_train[idx].astype('uint8'))\n","        plt.axis('off')\n","        if i == 0:\n","            plt.title(cls)\n","plt.show()"],"metadata":{"id":"DEY3lJpLhB_I"},"id":"DEY3lJpLhB_I","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The data is alreade split the into train, val, and test sets. In addition we will\n","# create a small development set as a subset of the training data;\n","# we can use this for development so our code runs faster.\n","\n","num_dev = 500\n","\n","# We will also make a development set, which is a small subset of\n","# the training set.\n","mask = np.random.choice(X_train.shape[0], num_dev, replace=False)\n","X_dev = X_train[mask]\n","y_dev = y_train[mask]\n","\n","\n","print('Train data shape: ', X_train.shape)\n","print('Train labels shape: ', y_train.shape)\n","print('Validation data shape: ', X_val.shape)\n","print('Validation labels shape: ', y_val.shape)\n","print('Test data shape: ', X_test.shape)\n","print('Test labels shape: ', y_test.shape)\n","print('Dev data shape: ', X_dev.shape)\n","print('Dev labels shape: ', y_dev.shape)"],"metadata":{"id":"7VYBhdyvhO58"},"id":"7VYBhdyvhO58","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Preprocessing: reshape the image data into rows\n","X_train = np.reshape(X_train, (X_train.shape[0], -1))\n","X_val = np.reshape(X_val, (X_val.shape[0], -1))\n","X_test = np.reshape(X_test, (X_test.shape[0], -1))\n","X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n","\n","# As a sanity check, print out the shapes of the data\n","print('Training data shape: ', X_train.shape)\n","print('Validation data shape: ', X_val.shape)\n","print('Test data shape: ', X_test.shape)\n","print('dev data shape: ', X_dev.shape)"],"metadata":{"id":"yhG-95tXh-M0"},"id":"yhG-95tXh-M0","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Preprocessing: subtract the mean image\n","# first: compute the image mean based on the training data\n","mean_image = np.mean(X_train, axis=0)\n","print(mean_image[:10]) # print a few of the elements\n","plt.figure(figsize=(4,4))\n","plt.imshow(mean_image.reshape((48,48,3)).astype('uint8')) # visualize the mean image\n","plt.show()\n","\n","# second: subtract the mean image from train and test data\n","X_train -= mean_image\n","X_val -= mean_image\n","X_test -= mean_image\n","X_dev -= mean_image\n","\n","# third: append the bias dimension of ones (i.e. bias trick) so that our SVM\n","# only has to worry about optimizing a single weight matrix W.\n","X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n","X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n","X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n","X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n","\n","print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)"],"metadata":{"id":"fhBUKuEXiFCK"},"id":"fhBUKuEXiFCK","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"4721c3cb","metadata":{"id":"4721c3cb"},"source":["## Softmax Classifier\n","\n","Your code for this section will all be written inside `classifiers/softmax.py`.\n"]},{"cell_type":"code","execution_count":null,"id":"d42765cd","metadata":{"id":"d42765cd"},"outputs":[],"source":["# First implement the naive softmax loss function with nested loops.\n","# Open the file classifiers/softmax.py and implement the\n","# softmax_loss_naive function.\n","\n","from classifiers.softmax import softmax_loss_naive\n","import time\n","\n","# Generate a random softmax weight matrix and use it to compute the loss.\n","W = np.random.randn(6913, 3) * 0.0001 \n","loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n","\n","# As a rough sanity check, our loss should be something close to -log(0.33).\n","print('loss: %f' % loss)\n","print('sanity check: %f' % (-np.log(0.33)))"]},{"cell_type":"markdown","id":"6f88764e","metadata":{"tags":["pdf-inline"],"id":"6f88764e"},"source":["**Inline Question**\n","\n","Include in the report why we expect our loss to be close to -log(0.33)? Explain briefly.**\n","\n"]},{"cell_type":"code","execution_count":null,"id":"4c2626ef","metadata":{"id":"4c2626ef"},"outputs":[],"source":["# Complete the implementation of softmax_loss_naive and implement a (naive)\n","# version of the gradient that uses nested loops.\n","loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n","\n","# As we did for the SVM, use numeric gradient checking as a debugging tool.\n","# The numeric gradient should be close to the analytic gradient.\n","from classifiers.Deep_learning_course.gradient_check import grad_check_sparse\n","f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n","grad_numerical = grad_check_sparse(f, W, grad, 10)\n","\n","# similar to SVM case, do another gradient check with regularization\n","loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n","f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n","grad_numerical = grad_check_sparse(f, W, grad, 10)"]},{"cell_type":"code","execution_count":null,"id":"434e757f","metadata":{"id":"434e757f"},"outputs":[],"source":["# Now that we have a naive implementation of the softmax loss function and its gradient,\n","# implement a vectorized version in softmax_loss_vectorized.\n","# The two versions should compute the same results, but the vectorized version should be\n","# much faster.\n","tic = time.time()\n","loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n","toc = time.time()\n","print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n","print('Naive gradient: '.format(grad_naive))\n","\n","from classifiers.softmax import softmax_loss_vectorized\n","tic = time.time()\n","loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n","toc = time.time()\n","print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n","print('Vectorized gradient: '.format(grad_vectorized))\n","\n","# As we did for the SVM, we use the Frobenius norm to compare the two versions\n","# of the gradient.\n","grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n","print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n","print('Gradient difference: %f' % grad_difference)"]},{"cell_type":"code","execution_count":null,"id":"2e5e374d","metadata":{"tags":["code"],"test":"tuning","id":"2e5e374d"},"outputs":[],"source":["# Use the validation set to tune hyperparameters (regularization strength and\n","# learning rate). You should experiment with different ranges for the learning\n","# rates and regularization strengths.\n","\n","from classifiers import Softmax\n","results = {}\n","best_val = -1\n","best_softmax = None\n","\n","################################################################################\n","# TODO:                                                                        #\n","# Use the validation set to set the learning rate and regularization strength. #\n","# This should be identical to the validation that you did for the SVM; save    #\n","# the best trained softmax classifer in best_softmax.                          #\n","################################################################################\n","\n","# Provided as a reference. You may or may not want to change these hyperparameters\n","learning_rates = [1e-7, 5e-7]\n","regularization_strengths = [2.5e4, 5e4]\n","\n","# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","pass\n","\n","# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    \n","# Print out results.\n","for lr, reg in sorted(results):\n","    train_accuracy, val_accuracy = results[(lr, reg)]\n","    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n","                lr, reg, train_accuracy, val_accuracy))\n","    \n","print('best validation accuracy achieved during cross-validation: %f' % best_val)"]},{"cell_type":"code","execution_count":null,"id":"deb37cc6","metadata":{"test":"test","id":"deb37cc6"},"outputs":[],"source":["# evaluate on test set\n","# Evaluate the best softmax on test set\n","y_test_pred = best_softmax.predict(X_test)\n","test_accuracy = np.mean(y_test == y_test_pred)\n","print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"]},{"cell_type":"markdown","id":"df501314","metadata":{"tags":["pdf-inline"],"id":"df501314"},"source":["**Inline Question** \n","\n","Consider the following. Include in your report if you think it adds value:\n","\n","Suppose the overall training loss is defined as the sum of the per-datapoint loss over all training examples. It is possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n","\n"]},{"cell_type":"code","execution_count":null,"id":"ade33adc","metadata":{"id":"ade33adc"},"outputs":[],"source":["# Visualize the learned weights for each class\n","w = best_softmax.W[:-1,:] # strip out the bias\n","w = w.reshape(48, 48, 3, 3)\n","\n","w_min, w_max = np.min(w), np.max(w)\n","\n","classes = ['bkl', 'vasc','nv']\n","for i in range(3):\n","    plt.subplot(1, 3, i + 1)\n","    \n","    # Rescale the weights to be between 0 and 255\n","    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n","    plt.imshow(wimg.astype('uint8'))\n","    plt.axis('off')\n","    plt.title(classes[i])"]},{"cell_type":"markdown","source":["**Inline Question**\n","\n","In the report, describe what your visualized weights look like, and offer a brief explanation for why they look the way they do in comparisson to the SVM weights.\n"],"metadata":{"id":"Zh-f7FgvY4Gx"},"id":"Zh-f7FgvY4Gx"},{"cell_type":"code","execution_count":null,"id":"4d5d17f8","metadata":{"id":"4d5d17f8"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":5}